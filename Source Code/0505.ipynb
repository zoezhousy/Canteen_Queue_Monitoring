{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people detected: 9\n",
      "New frame captured: 0\n",
      "Number of people detected: 9\n",
      "New frame captured: 1\n",
      "Number of people detected: 8\n",
      "New frame captured: 2\n",
      "Number of people detected: 7\n",
      "New frame captured: 3\n",
      "Number of people detected: 7\n",
      "New frame captured: 4\n",
      "Number of people detected: 11\n",
      "New frame captured: 5\n",
      "Number of people detected: 11\n",
      "New frame captured: 6\n",
      "Number of people detected: 8\n",
      "New frame captured: 7\n",
      "Number of people detected: 3\n",
      "New frame captured: 8\n",
      "Number of people detected: 5\n",
      "New frame captured: 9\n",
      "Number of people detected: 6\n",
      "New frame captured: 10\n",
      "Number of people detected: 8\n",
      "New frame captured: 11\n",
      "Number of people detected: 3\n",
      "New frame captured: 12\n",
      "Number of people detected: 6\n",
      "New frame captured: 13\n",
      "Number of people detected: 7\n",
      "New frame captured: 14\n",
      "Number of people detected: 8\n",
      "New frame captured: 15\n",
      "Number of people detected: 7\n",
      "New frame captured: 16\n",
      "Number of people detected: 5\n",
      "New frame captured: 17\n",
      "Number of people detected: 7\n",
      "New frame captured: 18\n",
      "Number of people detected: 5\n",
      "New frame captured: 19\n",
      "Number of people detected: 3\n",
      "New frame captured: 20\n",
      "Number of people detected: 2\n",
      "New frame captured: 21\n",
      "Number of people detected: 5\n",
      "New frame captured: 22\n",
      "Number of people detected: 4\n",
      "New frame captured: 23\n",
      "Number of people detected: 5\n",
      "New frame captured: 24\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Initialize HOG person detector\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "video_name = '/Users/tracyhanwenyu/Desktop/Dataset/VA1.mp4'\n",
    "cap = cv2.VideoCapture(video_name)\n",
    "\n",
    "time_skips = float(1000) #skip every 1 seconds. modify if need\n",
    "\n",
    "count = 0\n",
    "success,image = cap.read()\n",
    "while success:\n",
    "        \n",
    "    # Detect people in the image\n",
    "    boxes, weights = hog.detectMultiScale(image, winStride=(4, 4), padding=(8, 8), scale=1.05)\n",
    "\n",
    "    # Draw rectangle around each person\n",
    "    for (x, y, w, h) in boxes:\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "    \n",
    "    # save image\n",
    "    cv2.imwrite(\"/Users/tracyhanwenyu/Desktop/images/frame%d.jpg\" % count, image)\n",
    "    \n",
    "    # Count the number of people detected\n",
    "    num_people = len(boxes)\n",
    "    print('Number of people detected:', num_people)\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_MSEC, (count*time_skips))\n",
    "    print('New frame captured: ' + str(count))\n",
    "\n",
    "    # move the time\n",
    "    success,image = cap.read()\n",
    "    count += 1\n",
    "\n",
    "# release after reading    \n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v7.0-306-gb599ae42 Python-3.8.8 torch-2.2.2 CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete âœ… (8 CPUs, 16.0 GB RAM, 210.1/465.6 GB disk)\n",
      "if you want to use the default images, just click `Cancel upload`\n",
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s.pt'], source=/Users/tracyhanwenyu/Desktop/images, data=data/coco128.yaml, imgsz=[256, 256], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5 ðŸš€ v7.0-306-gb599ae42 Python-3.8.8 torch-2.2.2 CPU\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.1M/14.1M [00:01<00:00, 8.04MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "image 1/25 /Users/tracyhanwenyu/Desktop/images/frame0.jpg: 160x256 3 persons, 64.3ms\n",
      "image 2/25 /Users/tracyhanwenyu/Desktop/images/frame1.jpg: 160x256 3 persons, 54.7ms\n",
      "image 3/25 /Users/tracyhanwenyu/Desktop/images/frame10.jpg: 160x256 2 persons, 51.1ms\n",
      "image 4/25 /Users/tracyhanwenyu/Desktop/images/frame11.jpg: 160x256 4 persons, 1 bench, 52.7ms\n",
      "image 5/25 /Users/tracyhanwenyu/Desktop/images/frame12.jpg: 160x256 2 persons, 1 dining table, 52.2ms\n",
      "image 6/25 /Users/tracyhanwenyu/Desktop/images/frame13.jpg: 160x256 1 person, 51.9ms\n",
      "image 7/25 /Users/tracyhanwenyu/Desktop/images/frame14.jpg: 160x256 1 person, 49.6ms\n",
      "image 8/25 /Users/tracyhanwenyu/Desktop/images/frame15.jpg: 160x256 1 person, 1 bench, 52.4ms\n",
      "image 9/25 /Users/tracyhanwenyu/Desktop/images/frame16.jpg: 160x256 2 persons, 1 bench, 53.1ms\n",
      "image 10/25 /Users/tracyhanwenyu/Desktop/images/frame17.jpg: 160x256 2 persons, 51.5ms\n",
      "image 11/25 /Users/tracyhanwenyu/Desktop/images/frame18.jpg: 160x256 1 person, 1 bench, 102.1ms\n",
      "image 12/25 /Users/tracyhanwenyu/Desktop/images/frame19.jpg: 160x256 1 person, 1 bench, 51.0ms\n",
      "image 13/25 /Users/tracyhanwenyu/Desktop/images/frame2.jpg: 160x256 4 persons, 56.2ms\n",
      "image 14/25 /Users/tracyhanwenyu/Desktop/images/frame20.jpg: 160x256 1 person, 1 bench, 53.7ms\n",
      "image 15/25 /Users/tracyhanwenyu/Desktop/images/frame21.jpg: 160x256 2 persons, 50.5ms\n",
      "image 16/25 /Users/tracyhanwenyu/Desktop/images/frame22.jpg: 160x256 4 persons, 1 backpack, 51.1ms\n",
      "image 17/25 /Users/tracyhanwenyu/Desktop/images/frame23.jpg: 160x256 3 persons, 1 bench, 51.7ms\n",
      "image 18/25 /Users/tracyhanwenyu/Desktop/images/frame24.jpg: 160x256 3 persons, 62.8ms\n",
      "image 19/25 /Users/tracyhanwenyu/Desktop/images/frame3.jpg: 160x256 3 persons, 51.5ms\n",
      "image 20/25 /Users/tracyhanwenyu/Desktop/images/frame4.jpg: 160x256 3 persons, 52.6ms\n",
      "image 21/25 /Users/tracyhanwenyu/Desktop/images/frame5.jpg: 160x256 3 persons, 1 handbag, 53.0ms\n",
      "image 22/25 /Users/tracyhanwenyu/Desktop/images/frame6.jpg: 160x256 4 persons, 1 bench, 53.1ms\n",
      "image 23/25 /Users/tracyhanwenyu/Desktop/images/frame7.jpg: 160x256 4 persons, 1 bench, 54.5ms\n",
      "image 24/25 /Users/tracyhanwenyu/Desktop/images/frame8.jpg: 160x256 1 person, 56.3ms\n",
      "image 25/25 /Users/tracyhanwenyu/Desktop/images/frame9.jpg: 160x256 3 persons, 1 dining table, 57.2ms\n",
      "Speed: 0.4ms pre-process, 55.6ms inference, 1.4ms NMS per image at shape (1, 3, 256, 256)\n",
      "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /content\n",
    "!git clone https://github.com/ultralytics/yolov5\n",
    "%cd yolov5\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "from yolov5 import utils\n",
    "display = utils.notebook_init()\n",
    "\n",
    "print(\"if you want to use the default images, just click `Cancel upload`\")\n",
    "\n",
    "!python detect.py --weights yolov5s.pt --img 256 --conf 0.25 --source /Users/tracyhanwenyu/Desktop/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 1: Number of people: 3\n",
      "Frame 2: Number of people: 5\n",
      "Frame 3: Number of people: 3\n",
      "Frame 4: Number of people: 4\n",
      "Frame 5: Number of people: 4\n",
      "Frame 6: Number of people: 4\n",
      "Frame 7: Number of people: 4\n",
      "Frame 8: Number of people: 1\n",
      "Frame 9: Number of people: 2\n",
      "Frame 10: Number of people: 3\n",
      "Frame 11: Number of people: 3\n",
      "Frame 12: Number of people: 2\n",
      "Frame 13: Number of people: 1\n",
      "Frame 14: Number of people: 2\n",
      "Frame 15: Number of people: 1\n",
      "Frame 16: Number of people: 2\n",
      "Frame 17: Number of people: 3\n",
      "Frame 18: Number of people: 2\n",
      "Frame 19: Number of people: 1\n",
      "Frame 20: Number of people: 2\n",
      "Frame 21: Number of people: 3\n",
      "Frame 22: Number of people: 4\n",
      "Frame 23: Number of people: 4\n",
      "Frame 24: Number of people: 3\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture('/Users/tracyhanwenyu/Desktop/Dataset/VA1.mp4')\n",
    "\n",
    "# Initialize frame counter\n",
    "frame_counter = 0\n",
    "seconds = 0\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    # Set the position in the video to the current second\n",
    "    cap.set(cv2.CAP_PROP_POS_MSEC, seconds * 1000)\n",
    "    # Read the next frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "        frame_model = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_model = frame_model / 255.0\n",
    "        frame_model = np.transpose(frame_model, (2, 0, 1))\n",
    "        frame_model = torch.from_numpy(frame_model).float()\n",
    "\n",
    "        # Apply the Faster R-CNN model to the frame\n",
    "        output = model([frame_model])\n",
    "\n",
    "         # Apply non-maximum suppression\n",
    "        nms_indices = torchvision.ops.nms(output[0]['boxes'], output[0]['scores'], 0.3)\n",
    "        output[0]['boxes'] = output[0]['boxes'][nms_indices]\n",
    "        output[0]['labels'] = output[0]['labels'][nms_indices]\n",
    "\n",
    "        # Filter out the detections with low confidence scores\n",
    "        high_conf_indices = [i for i, score in enumerate(output[0]['scores']) if score > 0.7]\n",
    "        output[0]['boxes'] = output[0]['boxes'][high_conf_indices]\n",
    "        output[0]['labels'] = output[0]['labels'][high_conf_indices]\n",
    "\n",
    "        # Count the number of people detected in the frame\n",
    "        num_people = sum(1 for box, label in zip(output[0]['boxes'], output[0]['labels']) if label == 1)\n",
    "\n",
    "        # Draw bounding boxes around the detected people\n",
    "        for box, label in zip(output[0]['boxes'], output[0]['labels']):\n",
    "            if label == 1:\n",
    "                cv2.rectangle(frame, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)\n",
    "        \n",
    "        # Save the frame\n",
    "        cv2.imwrite(f'/Users/tracyhanwenyu/Desktop/images/frame{frame_counter}.jpg', frame)\n",
    "\n",
    "        # Increment frame counter\n",
    "        frame_counter += 1\n",
    "        seconds += 1\n",
    "\n",
    "        # Print the number of people detected in the frame along with the frame number\n",
    "        print(f'Frame {frame_counter}: Number of people: {num_people}')\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release the video capture\n",
    "cap.release()\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
